{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"VbfAPs-7YE8C"},"outputs":[],"source":["import os\n","from tqdm import tqdm\n","\n","import copy\n","import pandas as pd\n","import numpy as np\n","\n","import random\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import statistics as st\n","\n","from data import *\n","from module import *\n","from utils import *"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1O2DhIILYE8C"},"outputs":[],"source":["# Define Dataset Information\n","train_dataset_file = './data/diabetes_train.csv'\n","synthesized_file = './data/diabetes_cgtgan.csv'\n","\n","num_columns = 9\n","\n","continuous_column_names = [\n","    1, # Pregnancies\n","    2, # Glucose\n","    3, # Blood Pressure\n","    4, # Skin Thickness\n","    5, # Insulin\n","    6, # BMI\n","    7, # Diabetes\n","    8, # Age\n","]\n","mixed_column_names = []\n","mixed = {}\n","categorical_column_names = [\n","    0, #  Outcome\n","]\n","integer_columns_names = [\n","]\n","target_col = 0\n","\n","problem_type = 'Classification'\n","regression_col = []\n","classification_col = [target_col]\n","\n","continuous_columns_wo_target = [\n","    1, \n","    2, \n","    3, \n","    4, \n","    5, \n","    6, \n","    7, \n","    8\n","]\n","category_columns_wo_target = []\n","classifiers_for_utility = ['lr', 'dt', 'rf']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9OJS2w7DYE8D"},"outputs":[],"source":["df_train = pd.read_csv(train_dataset_file)\n","\n","for i in range(len(df_train.columns)):\n","    cur_col_name = df_train.columns[i]\n","    df_train.rename(columns = {cur_col_name:i}, inplace = True)\n","\n","if problem_type == 'Classification':\n","    df_val = df_train.groupby(target_col).sample(frac = 0.2)\n","else:\n","    df_val = df_train.sample(frac = 0.2)\n","\n","df_val = df_val.reset_index(drop=True)\n","df_train = df_train.reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Rgl-LhQYE8D"},"outputs":[],"source":["# Hyperparameters\n","batch_size = 256\n","rand_dim = 100\n","\n","embed_dim = 64\n","cond_dim = embed_dim * num_columns\n","\n","learning_rate = 1e-4\n","num_updates = 5000\n","\n","num_critic_iters = 5\n","lambda_gp = 10\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DdnzDiJpYE8D"},"outputs":[],"source":["# preprocess dataset\n","df_train_max_min_norm, max_list, min_list = df_min_max_norm(df_train, continuous_column_names)\n","\n","num_mode_list = []\n","\n","for _ in range(len(continuous_column_names)):\n","    num_mode_list.append(5)\n","\n","preprocessor = Preprocessor(data= df_train_max_min_norm, continuous_columns = continuous_column_names,\n","                            mixed=mixed, categorical_columns=categorical_column_names,\n","                            skew_columns=[],\n","                            integer_columns = integer_columns_names,\n","                            num_mode_list = num_mode_list)\n","\n","preprocessed_data = preprocessor.fit_transform(preprocessor.data)\n","\n","embedding = CategoricalEmbedding(metadata=preprocessor.metadata)\n","\n","embedding_data = embedding.embed(preprocessed_data)\n","\n","merge = MergeDataset(embedding_data, embedding.metadata)\n","\n","merge_data = merge.merge()\n","\n","merge_tensor = torch.from_numpy(merge_data).float().to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4023555,"status":"ok","timestamp":1722334132952,"user":{"displayName":"Occasion Lee","userId":"16162193442526470085"},"user_tz":-540},"id":"jpvmWsN_YE8D","outputId":"99e56b33-19d2-4f46-e887-969c4d19848d"},"outputs":[],"source":["seed = 3\n","\n","# Declare modules\n","deterministic = True\n","\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","if deterministic:\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","# declare DataSampler\n","datasampler = DataSampler(data=df_train_max_min_norm, metadata = embedding.metadata)\n","\n","# declare module for projection\n","projection = Projection_virtual_node(input_dim = merge_tensor.shape[1], metadata=merge.metadata, emb_dim=embed_dim, device = device)\n","projection.to(device)\n","output_info_list = output_info_gather(projection.metadata)\n","\n","# declare generator\n","cat_class_num_list = []\n","for detail in projection.metadata['details']:\n","    if detail['type'] == 'category':\n","        cat_class_num_list.append(detail['n'])\n","\n","generator = NodewiseGenerator(metadata = preprocessor.metadata,\n","                            rand_dim = rand_dim + embed_dim,\n","                            num_nodes = num_columns + 1,\n","                            embed_dim = embed_dim,\n","                            device=device,\n","                            cat_class_num_list = cat_class_num_list)\n","generator.to(device)\n","\n","# declare critic\n","critic_layers = GCN(embed_dim, num_columns+1, device, 0)\n","critic = Critic(layers=critic_layers, dim_representation=cond_dim+embed_dim, device = device)\n","critic.to(device)\n","\n","# declare classifier\n","classifier = Classifier(metadata=projection.metadata, embed_dim=embed_dim, num_nodes = num_columns+1, num_mode_list = num_mode_list)\n","classifier.to(device)\n","\n","# declare adj\n","adj = nn.Linear(in_features=num_columns+1, out_features=num_columns+1)\n","adj.weight.data = (torch.ones(size = (num_columns + 1, num_columns + 1)) - torch.eye(num_columns + 1)) / math.sqrt(num_columns + 1)\n","adj.to(device)\n","\n","# declare optimizers\n","optimizer_c = optim.Adam(critic.parameters(), lr=learning_rate, weight_decay = 1e-7)\n","optimizer_g = optim.Adam(generator.parameters(), lr=learning_rate, weight_decay = 1e-7)\n","optimizer_p = optim.Adam(projection.parameters(), lr = learning_rate, weight_decay = 1e-7)\n","optimizer_cl = optim.Adam(classifier.parameters(), lr=learning_rate, weight_decay = 1e-7)\n","optimizer_adj = optim.Adam(adj.parameters(), lr=learning_rate, weight_decay = 1e-7)\n","\n","# declare schedulers\n","scheduler_c = optim.lr_scheduler.ExponentialLR(optimizer = optimizer_c, gamma = 0.9)\n","scheduler_g = optim.lr_scheduler.ExponentialLR(optimizer = optimizer_g, gamma = 0.9)\n","scheduler_p = optim.lr_scheduler.ExponentialLR(optimizer = optimizer_p, gamma = 0.9)\n","scheduler_cl = optim.lr_scheduler.ExponentialLR(optimizer = optimizer_cl, gamma = 0.9)\n","scheduler_adj = optim.lr_scheduler.ExponentialLR(optimizer = optimizer_adj, gamma = 0.9)\n","\n","# Train\n","list_real_vs_fake, projection, generator, critic, classifier, adj, df_fake = train_concat(\n","    train_dataset = merge_tensor,\n","    generator=generator,\n","    critic=critic,\n","    classifier = classifier,\n","    projection=projection,\n","    datasampler=datasampler,\n","    preprocessor=preprocessor,\n","    batch_size=batch_size,\n","    rand_dim=rand_dim,\n","    total_embed_dim=preprocessor.metadata[\"total_embed_dim\"],\n","    optimizer_c=optimizer_c, optimizer_g=optimizer_g, optimizer_p=optimizer_p, optimizer_cl=optimizer_cl, optimizer_adj=optimizer_adj,\n","    scheduler_c=scheduler_c, scheduler_g=scheduler_g, scheduler_p=scheduler_p, scheduler_cl=scheduler_cl, scheduler_adj=scheduler_adj,\n","    num_updates=num_updates,\n","    num_critic_iters=num_critic_iters,\n","    lambda_gp=lambda_gp,\n","    device=device,\n","    output_info_list=output_info_list, continuous_column_names=continuous_column_names, categorical_column_names=categorical_column_names,\n","    target_col=target_col,\n","    node_classification_col=classification_col, node_regression_col=regression_col,\n","    adj = adj,\n","    problem_type=problem_type,\n","    df_train = df_train, df_test = df_val,\n","    max_list = max_list, min_list = min_list,\n","    continuous_columns_wo_target= continuous_columns_wo_target,\n","    category_columns_wo_target= category_columns_wo_target,\n","    classifiers_utility= classifiers_for_utility,\n","    )\n","\n","# Save\n","df_fake.to_csv(synthesized_file, index=False)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.8"}},"nbformat":4,"nbformat_minor":0}
